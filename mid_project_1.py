# -*- coding: utf-8 -*-
"""Mid-Project-1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1E-TWpTlk-X6011y-sfxfKOE4uiDc2A5t
"""

import pandas as pd 


train_features = pd.read_csv('train.csv')
test_features = pd.read_csv('test.csv')
df_features = train_features.append(test_features,ignore_index=True)


train_labels = pd.read_csv('train_label.csv', header=None, names=['Total_Booking'])
test_labels = pd.read_csv('test_label.csv', header=None, names=['Total_Booking'])
df_labels = train_labels.append(test_labels,ignore_index=True)


df = pd.concat([df_features, df_labels], axis=1)

df.info()

df.head()

df.isnull().sum()

#no null values in the dataset

df.dtypes

import seaborn as sns
sns.boxplot(x=df['windspeed'])
#outlier analysis

import matplotlib.pyplot as plt
import matplotlib
fig, ax = plt.subplots(figsize=(16,8))
ax.scatter(df['windspeed'], df['Total_Booking'])
ax.set_xlabel('Wind Speed')
ax.set_ylabel('Total Booking')
plt.show()

sns.boxplot(df['Total_Booking'],color='r')

from scipy import stats
import numpy as np
z = np.abs(stats.zscore(df[['windspeed','temp','atemp','humidity','Total_Booking']]))
print(z)

threshold = 3
print(np.where(z > 3))
#outlier rows

Q1 = df.quantile(0.25)
Q3 = df.quantile(0.75)
IQR = Q3 - Q1
print(IQR)

iqr = (df < (Q1 - 1.5 * IQR)) |(df > (Q3 + 1.5 * IQR))

iqr.head(10)
#True indicates presence of outlier 
#False indicates Absence of outlier

#iqr and Z-scores are similar in removing outliers 
df = df[(z < 3).all(axis=1)]
df.head(10)
#5th row had a outlier, with this we have removed such rows

sns.boxplot(x=df['windspeed'])
#compared to last plot, we have less outliers

sns.boxplot(df['Total_Booking'],color='r')

sns.countplot(x='weather',data=df)
plt.xticks(rotation=90)
plt.show()

sns.countplot(x='season',data=df,hue='weather')
plt.legend(bbox_to_anchor=(1.01, 1),borderaxespad=0)
plt.show()

sns.countplot(x='season',data=df,hue='holiday')
new_labels = ['No Holiday', 'Holiday']
plt.legend(bbox_to_anchor=(1.01, 1),borderaxespad=0,labels=new_labels)
plt.show()
#winter had more holidays

sns.countplot(x='season',data=df,hue='workingday')
new_labels = ['No Workingday', 'Workingday']
plt.legend(bbox_to_anchor=(1.01, 1),borderaxespad=0,labels=new_labels)
plt.show()
#winter had more holidays

Not_Holiday_Not_workingday=df[(df.holiday==0) & (df.workingday==0)].shape[0]
print('No working and No Holiday = ', Not_Holiday_Not_workingday)
Holiday=df[(df.holiday==1)].shape[0]
print('Total HoliDay = ', Holiday)
WorkingDay=df[(df.workingday==1)].shape[0]
print('Total Working Day = ', WorkingDay)
plt.pie(x=[Not_Holiday_Not_workingday,WorkingDay,Holiday],labels=['No Holiday & No workingday','WorkingDay','Holiday'],explode=(.1,.1,.1),colors=['r','g','b'],autopct='%.2f',wedgeprops={'edgecolor':'k'})
plt.show()

df['Year'] = pd.DatetimeIndex(df['datetime']).year
df['Month'] = pd.DatetimeIndex(df['datetime']).month_name()
df['Day'] = pd.DatetimeIndex(df['datetime']).day_name()
df['Hour'] = pd.DatetimeIndex(df['datetime']).hour

df['timing'] = pd.to_datetime(df.datetime, format='%m/%d/%Y %H:%M')
a = df.assign(dept_session=pd.cut(df.timing.dt.hour,[0,6,12,18,24],labels=['Night','Morning','Afternoon','Evening']))
df['booking_session'] = a['dept_session']

sns.countplot(x='Month',data=df )
plt.xticks(rotation=90)
plt.show()

sns.countplot(x='Day',data=df)
plt.xticks(rotation=90)
plt.show()
#saturdays have more counts

sns.countplot(x='Year',data=df,hue='booking_session')
plt.legend(bbox_to_anchor=(1.01, 1),borderaxespad=0)
plt.xticks(rotation=90)
plt.show()
#evenings have less counts compared to rest of the parts of day

sns.countplot(x="booking_session", data=df)

sns.lineplot(x='Month', y="Total_Booking", data=df, sort=True)
plt.xticks(rotation=90)
#january and february have less booking counts probably due to cold weather

sns.lineplot(x="Day", y="Total_Booking", data=df)
plt.xticks(rotation=90)

sns.lineplot(x="Hour", y="Total_Booking", data=df)
plt.xticks(rotation=90)

sns.lineplot(x="temp", y="Total_Booking", data=df)
plt.xticks(rotation=90)

sns.lineplot(x="atemp", y="Total_Booking", data=df)
plt.xticks(rotation=90)

sns.lineplot(x="windspeed", y="Total_Booking", data=df)
plt.xticks(rotation=90)

sns.lineplot(x="humidity", y="Total_Booking", data=df)
plt.xticks(rotation=90)

corr = df.corr()

corr

f, ax = plt.subplots(figsize=(11, 6))
mask = np.triu(np.ones_like(corr, dtype=bool))
cmap = sns.diverging_palette(230, 20, as_cmap=True)
sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,
            square=True, linewidths=.5, cbar_kws={"shrink": .5}, annot=True)
plt.show()

sns.pairplot(df, kind="scatter")
plt.show()

##Task -2

#Tasks to do
'''
1) Add the hour day month and year coloumn to train and Test and remove datetime coloumn
2) convert categorical to numerical 
3) Concantenate the features and labels
4) Remove junk rows where working =0  and holiday = 0 and vice versa
5) Remove the outliers i.e, Reduce the Total_Booking to 600
6) Scale the values
7) remove highly co related coloums 

'''

train_features.head(10)

test_features.head(10)

train_features['datetime']=pd.to_datetime(train_features['datetime'])
test_features['datetime']=pd.to_datetime(test_features['datetime'])

train_features['Year'] = pd.DatetimeIndex(train_features['datetime']).year
train_features['Month'] = pd.DatetimeIndex(train_features['datetime']).month
train_features['Day'] = pd.DatetimeIndex(train_features['datetime']).day
train_features['Hour'] = pd.DatetimeIndex(train_features['datetime']).hour

test_features['Year'] = pd.DatetimeIndex(test_features['datetime']).year
test_features['Month'] = pd.DatetimeIndex(test_features['datetime']).month
test_features['Day'] = pd.DatetimeIndex(test_features['datetime']).day
test_features['Hour'] = pd.DatetimeIndex(test_features['datetime']).hour


## Adding Hour Day Month and Year to Train features and Test features

train_features=train_features.drop('datetime',axis=1)
test_features=test_features.drop('datetime',axis=1)

# Drop Date and time coloumn from Train features and test features

from sklearn.preprocessing import LabelEncoder

labelencoder= LabelEncoder() 
train_features['season']= labelencoder.fit_transform(train_features['season'])
train_features['weather']= labelencoder.fit_transform(train_features['weather'])

test_features['season']= labelencoder.fit_transform(test_features['season']) 
test_features['weather']= labelencoder.fit_transform(test_features['weather'])

#Label Encoding the season and weather coulmns

#Combine features and Labels
train = pd.concat([train_features, train_labels], axis=1)
test = pd.concat([test_features, test_labels], axis=1)

# Have Only Rows with Holiday = 1 and Working Day = 0 and vice versa....remaining is junk data
train = train[((train['holiday']==1)&(train['workingday']==0))|((train['holiday']==0)&(train['workingday']==1))]
test = test[((test['holiday']==1)&(test['workingday']==0))|((test['holiday']==0)&(test['workingday']==1))]

#outlier removal
train.loc[train['Total_Booking']>600, 'Total_Booking']=np.mean(train['Total_Booking'])
test.loc[test['Total_Booking']>600, 'Total_Booking']=np.mean(test['Total_Booking'])

##there were major outliers in Total Booking coloumn

train.head()

test.head()

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
df = scaler.fit_transform(train)
train = pd.DataFrame(df,columns=train.columns)
train.head()

df1 = scaler.fit_transform(test)
test = pd.DataFrame(df1,columns=test.columns)
test.head()

plt.figure(figsize=(12,6))

corr = train.corr()
ax = sns.heatmap(corr, xticklabels=corr.columns, yticklabels=corr.columns,annot=True, linewidths=.2)

## atemp and temp coloums are co related we can remove any one ove the coloumn

train=train.drop('atemp',axis=1)
test=test.drop('atemp',axis=1)

train.columns

train.describe()

train.info()

##No Null Values, So we are good to Go With Learning

from sklearn.linear_model import LinearRegression 
from sklearn.tree import DecisionTreeRegressor
from sklearn.linear_model import Ridge, Lasso
from sklearn.model_selection import KFold
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_squared_error
from sklearn.metrics import accuracy_score
from sklearn.metrics import make_scorer
from math import sqrt
from sklearn.model_selection import GridSearchCV

xtrain=train.drop('Total_Booking', axis=1)
ytrain=train.Total_Booking

xtest=test.drop('Total_Booking', axis=1)
ytest=test.Total_Booking

xtrain.head()

ytest.head()

xtest.head()

ytest.head()

#K Fold Cross validation to use for each model 

def cross_validation_score(ml_model,cols = xtrain.columns):
    i = 1
    cv_scores=[]
   
    kf = KFold(n_splits=5,random_state=42,shuffle=True)
    for train_index,test_index in kf.split(xtrain,ytrain):
        print('\n{} of kfold {}'.format(i,kf.n_splits))
        model = ml_model
        model.fit(xtrain, ytrain)
        pred_val = model.predict(xtest)
        
        rmse_score = sqrt(mean_squared_error(ytest, pred_val))
        msg = ""
        msg += "RMSE score: {}".format(rmse_score)
        print("{}".format(msg))
        
        
        # Save scores
        cv_scores.append(rmse_score)
        i+=1
    return cv_scores

#Linear Regression
LR_scores = cross_validation_score(LinearRegression())

#Decision Trees
DT_scores = cross_validation_score(DecisionTreeRegressor(min_samples_leaf=5, min_samples_split=20))

#KNeighborsRegressor

KNN_scores = cross_validation_score(KNeighborsRegressor(n_neighbors=2))

#Ridge
ridge_scores = cross_validation_score(Ridge(alpha=1.0))

#Lasso
lasso_scores = cross_validation_score(Lasso(alpha=0.1))

regression_scores =  pd.DataFrame({'Linear regression':LR_scores, 'Decision Tree': DT_scores, 'KNN':KNN_scores, 
                             'Ridge':ridge_scores,'Lasso':lasso_scores})

regression_scores

##RMSE IN DECISIOIN TREE IS LESS COMPARED TO OTHER REGRESSORS

from sklearn.model_selection import GridSearchCV

DT = DecisionTreeRegressor()

parameters = {'max_depth': [2, 3, 5, 10], 
              'min_samples_split': [2, 3, 5],
              'min_samples_leaf': [1,5,8],
              'splitter': ["best", "random"],
             }


acc_scorer = make_scorer(mean_squared_error )

grid_obj = GridSearchCV(DT, parameters)
grid_obj = grid_obj.fit(xtrain, ytrain)

#

clf = grid_obj.best_estimator_

clf.fit(xtrain, ytrain)

predictions = clf.predict(xtest)
rmse_score = sqrt(mean_squared_error(ytest, predictions))
print(rmse_score)

from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor
from xgboost import XGBRegressor

rf_params = {'random_state': 24, 'n_estimators': 25, 'max_depth': 20, 'n_jobs': -1, "min_samples_split": 2}
rf_scores = cross_validation_score(RandomForestRegressor(**rf_params))

ada_scores = cross_validation_score(AdaBoostRegressor(n_estimators=50, random_state=0))

xgb_scores= cross_validation_score(XGBRegressor(max_depth=15,learning_rate=0.1,n_estimators=50, min_child_weight=5, random_state=42,objective='reg:squarederror',reg_lambda=5))

ensembled_scores = pd.DataFrame({'Random Forest':rf_scores, 'AdaBoost Regressor':ada_scores,'XGBRegressor':xgb_scores})
ensembled_scores.head()

# 1) XGBoost Regressor has less RMSE
# 2) Random Forest is second in having less rmse scores

ensembled_scores.plot(kind="bar")

